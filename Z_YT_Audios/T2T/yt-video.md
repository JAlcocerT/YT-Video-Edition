Learn how to monitor LLMs workload like a Pro.

We are going to use the NetData, together with the Ollama project, and we are going to SelfHost it with Docker. Then, we will see how our infrastructure is doing when we are utilizing our local LLMs.

Feel free to deploy this at home, or at the cloud - same process applies.


🔗🔗🔗 Stack to deploy NetData: https://fossengineer.com/selfhosting-server-monitoring-with-netdata-and-docker/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

💻 Used During The Video
LLM used: https://ollama.ai/library/mistral
NetData Docker Image 🐋: https://hub.docker.com/r/netdata/netdata


🌟 Resources you might be Interested 🌟

* https://fossengineer.com/tags/gen-ai/
* SelfHost NGINX to provide https to Ollama: https://fossengineer.com/selfhosting-nginx-proxy-manager-docker/

* LLama-Index (Pypi): https://pypi.org/project/llama-index/
* Ollama + LogSec: https://fossengineer.com/selfhosting-logseq/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

👉 What Software have I used during the video?

It is all Free Software that you can install in Windows/Mac/Linux:

Docker: https://docs.docker.com/get-docker/
Portainer to manage the containers with UI: https://fossengineer.com/selfhosting-portainer-docker/
VSCodium to create the Dockerfile: https://vscodium.com/
OBS Studio to record screen: https://jalcocert.github.io/Linux/docs/nix/fav-apps/#dev--content

📚 Programming Books & Merch 📚
🐍 The Python Bible Book: https://www.neuralnine.com/books/
💻 The Algorithm Bible Book: https://www.neuralnine.com/books/
👕 Programming Merch: https://www.neuralnine.com/shop

🌐 Social Media & Contact 🌐 
📱 Website: https://www.neuralnine.com/
📷 Instagram:   / neuralnine  
🐦 Twitter:   / neuralnine  
🤵 LinkedIn:   / neuralnine  
📁 GitHub: https://github.com/NeuralNine 
🎙 Discord:   / discord  

🎵 Outro Music From: https://www.bensound.com/
🎵 Audio created by OpenAI API - onyx, TTS1 Model.